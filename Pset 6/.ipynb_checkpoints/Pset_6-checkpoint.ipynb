{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 156a - Problem Set 6\n",
    "\n",
    "## Patrone Samuel, 2140749\n",
    "\n",
    "The following notebook is publicly available at https://github.com/spatrone/CS156A-Caltech.git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "### Answer: [b] In general, deterministic noise will increase.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "Given that we are using a less complex hypothesis in $\\mathcal{H}^\\prime\\subset\\mathcal{H}$, we expect the best fit $g^\\prime\\in\\mathcal{H}^\\prime$ in this less complex hypothesis space to have an higher deterministic noise since there will be more of the target function that cannot be captured by it. In more mathematical terms, the deterministic noise is represented by the bias, which naturally increases for less complex hypothesis. The bias can be seen as the asymptotic value at which both $E_{in}$ and $E_{out}$ converge for $N\\to\\infty$ (in the case of zero stochastic noise) which is lower for more complex hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "### Answer: [a] $0.03,\\,0.08$\n",
    "\n",
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#import data\n",
    "\n",
    "training_set=pd.read_csv('in.dta',header=None,delim_whitespace=True)\n",
    "testing_set=pd.read_csv('out.dta',header=None,delim_whitespace=True)\n",
    "\n",
    "train_pts=training_set[[0, 1]].to_numpy()\n",
    "train_y=training_set[2].to_numpy()\n",
    "\n",
    "test_pts=testing_set[[0, 1]].to_numpy()\n",
    "test_y=testing_set[2].to_numpy()\n",
    "\n",
    "\n",
    "def color_pts(y):\n",
    "    #green is +1, red is -1\n",
    "    col=[]\n",
    "    for i in range(len(y)):\n",
    "        if(y[i]>0): col.append('green')\n",
    "        else: col.append('red')\n",
    "    return col\n",
    "\n",
    "def plot_pts(pts,y):\n",
    "    col=color_pts(y)\n",
    "    plt.scatter(pts[:,0],pts[:,1],color=col)\n",
    "    #plt.xlim([-1, 1])\n",
    "    #plt.ylim([-1, 1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#non-linear transformation\n",
    "\n",
    "def transform(pts):\n",
    "    res=[]\n",
    "    for i in range(len(pts)):\n",
    "        x1=pts[i][0]\n",
    "        x2=pts[i][1]\n",
    "        res.append([1,x1,x2,x1**2,x2**2,x1*x2,np.abs(x1-x2),np.abs(x1+x2)])\n",
    "    return np.array(res)\n",
    "        \n",
    "def lin_reg_w(X,y):\n",
    "    return np.dot(np.linalg.pinv(X),y)\n",
    "\n",
    "def h(pts,w):\n",
    "    return np.sign(np.dot(w,pts.T))\n",
    "\n",
    "def lin_reg(train_pts,train_y,test_pts,test_y,res=True):\n",
    "    N_train=len(train_pts)\n",
    "    N_test=len(test_pts)\n",
    "    \n",
    "    w=lin_reg_w(train_pts,train_y)\n",
    "    \n",
    "    #Ein computation\n",
    "    gin=h(train_pts,w)\n",
    "    testgin=(gin==train_y)\n",
    "    Ein=len(np.where(testgin==False)[0])/N_train\n",
    "    \n",
    "    #Eout computation\n",
    "    gout=h(test_pts,w)\n",
    "    testgout=(gout==test_y)\n",
    "    Eout=len(np.where(testgout==False)[0])/N_test\n",
    "    \n",
    "    #print results\n",
    "    if(res==True):\n",
    "        print(f'Linear Regression results:\\nEin={Ein:.2f}\\nEout={Eout:.2f}')\n",
    "    \n",
    "    return w,Ein,Eout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression results:\n",
      "Ein=0.03\n",
      "Eout=0.08\n"
     ]
    }
   ],
   "source": [
    "train_pts_transf=transform(train_pts)\n",
    "test_pts_transf=transform(test_pts)\n",
    "\n",
    "ex2=lin_reg(train_pts_transf,train_y,test_pts_transf,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems 3-6\n",
    "\n",
    "### Answers: [d] $[0.03,\\,0.08]$, [e] $[0.4,\\,0.4]$, [d] $-1$, [b] $0.06$\n",
    "\n",
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_w_lam(X,y,lam):\n",
    "    pinv_decay=np.dot(np.linalg.inv(np.dot(X.T,X)+lam*np.identity(len(X.T))),X.T)\n",
    "    return np.dot(pinv_decay,y)\n",
    "\n",
    "def lin_reg_wdecay(train_pts,train_y,test_pts,test_y,lam,res=True):\n",
    "    N_train=len(train_pts)\n",
    "    N_test=len(test_pts)\n",
    "    \n",
    "    w=lin_reg_w_lam(train_pts,train_y,lam)\n",
    "    \n",
    "    #Ein computation\n",
    "    gin=h(train_pts,w)\n",
    "    testgin=(gin==train_y)\n",
    "    Ein=len(np.where(testgin==False)[0])/N_train\n",
    "    \n",
    "    #Eout computation\n",
    "    gout=h(test_pts,w)\n",
    "    testgout=(gout==test_y)\n",
    "    Eout=len(np.where(testgout==False)[0])/N_test\n",
    "    \n",
    "    #print results\n",
    "    if(res==True):\n",
    "        print(f'Linear Regression results with k={np.log10(lam):.0f}:\\nEin={Ein:.2f}\\nEout={Eout:.2f}')\n",
    "    \n",
    "    return w,Ein,Eout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>Ein</th>\n",
       "      <th>Eout</th>\n",
       "      <th>w.w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>15.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>30.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>33.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>33.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  Ein  Eout   w.w\n",
       "0  4 0.43  0.45  0.00\n",
       "1  3 0.37  0.44  0.00\n",
       "2  2 0.20  0.23  0.03\n",
       "3  1 0.06  0.12  0.50\n",
       "4  0 0.00  0.09  2.52\n",
       "5 -1 0.03  0.06 15.37\n",
       "6 -2 0.03  0.08 30.39\n",
       "7 -3 0.03  0.08 33.40\n",
       "8 -4 0.03  0.08 33.74"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=[4,3,2,1,0,-1,-2,-3,-4]\n",
    "Ein=[]\n",
    "Eout=[]\n",
    "wnorm=[]\n",
    "\n",
    "for i in k:\n",
    "    ex5,Ein5,Eout5=lin_reg_wdecay(train_pts_transf,train_y,test_pts_transf,test_y,10**(i),res=False)\n",
    "    Ein.append(Ein5)\n",
    "    Eout.append(Eout5)\n",
    "    wnorm.append(np.dot(ex5,ex5))\n",
    "    \n",
    "    \n",
    "pd.options.display.float_format = '{:,.2f}'.format   \n",
    "pd.DataFrame(list(zip(k, Ein,Eout,wnorm)), columns =['k', 'Ein','Eout','w.w'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7\n",
    "\n",
    "### Answer: [c] $\\mathcal{H}(10,0,3)\\cap \\mathcal{H}(10,0,4)=\\mathcal{H}_2$\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "We first observe that the following identity holds:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{H}(Q,C=0,Q_0)=\\mathcal{H}_{Q_0-1}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "where $Q \\ge Q_0$.\n",
    "\n",
    "Furthermore, by definition we have that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{H}_{N-1}\\subset\\mathcal{H}_{N}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, \n",
    "\\begin{equation}\n",
    "\\mathcal{H}(10,0,3)\\cap \\mathcal{H}(10,0,4)=\\mathcal{H}_2\\cap \\mathcal{H}_3=\\mathcal{H}_2\\,.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8\n",
    "\n",
    "### Answer: [d] $45$\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "One single iteration of the backpropagation algorithm using one data point starts with the forward propagation, i.e. the computation of the neural network hypothesis which is obtained by computing the output vector at each layer\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{(l)}=\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "\\theta(\\mathbf{s}^{(l)})\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{s}^{(l)}$ is a vector whose component are\n",
    "\n",
    "\\begin{equation}\n",
    "s_i^{(l)}=\\sum_{j=0}^{d^{(l-1)}}w_{ij}^{(l)}x_j^{(l-1)}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "For each layer $l$, there are $d^{(l)}(d^{(l-1)}+1)$ multiplications to do, which is exactly the number of weights between the layer $l-1$ and the layer $l$. Hence, for a complete forward propagation, we have $N_w$ total multiplications, where\n",
    "\n",
    "\\begin{equation}\n",
    "N_w=\\sum_{l=1}^L d^{(l)}(d^{(l-1)}+1)\\,.\n",
    "\\end{equation}\n",
    "\n",
    "In the stochastic gradient descent algorithm, the weights are updated by taking a step in the negative gradient direction. To compute the gradient, we have to take the derivatives of the error function with respect to each single weight $w_{ij}^{(l)}$, which is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial e}{\\partial w_{ij}^{(l)}} = x^{(l-1)}_i \\delta^{(l)}_j\\,.\n",
    "\\end{equation}\n",
    "\n",
    "There are as many derivatives as weights, so we have other $N_w$ operations. Finally, the sensitivity $\\delta^{(l)}_j$ can be computed using the backpropagation algorithm by recursion with the following formula\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^{(l)}_j=\\theta^\\prime(s^{(l)}_j)\\sum_{k=1}^{d^{(l+1)}} w_{jk}^{(l+1)}\\delta^{(l+1)}_k\\,.\n",
    "\\end{equation}\n",
    "where $\\delta^{(L)}=2(x^{(L)}-y)\\theta^\\prime(s^{(L)})$.\n",
    "\n",
    "There are $L-1$ sensitivities to compute, for a total number of additional operations equal to\n",
    "\n",
    "\\begin{equation}\n",
    "N_b=\\sum_{l=1}^{L-1} d^{(l)}d^{(l+1)}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the total number of multiplications needed to carry out a single iteration of backpropagation is $N_{tot}=2N_w+N_b$, being $N_w$ the number of weights of the network and $N_b$ the number of operations needed to compute the sensitivities.\n",
    "\n",
    "In the example given, $d^{(0)}=5$, $d^{(1)}=3$, $d^{(2)}=1$. Hence, $N_w=22$, $N_b=3$ and $N_{tot}=47$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 9\n",
    "\n",
    "### Answer: [a] $46$\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "To minimize the number of weights, we want to reduce the number of connections. In order to achieve that, we consider the case in which the 36 units are equally distributed in 18 hidden layers of 2 units each.\n",
    "\n",
    "The following code allows as to compute the number of weights in this case, which turns out to be 46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "def nweights(d):\n",
    "    w=0\n",
    "    for i in range(len(d)-1):\n",
    "        w+=(d[i]+1)*d[i+1]\n",
    "    return w\n",
    "\n",
    "d=[9]\n",
    "for i in range(19):\n",
    "    d.append(1)\n",
    "\n",
    "print(nweights(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 10\n",
    "\n",
    "### Answer: [e] $510$ \n",
    "\n",
    "### Derivation:\n",
    "\n",
    "To solve this problem, we will analytically compute the number of weights for all the possible configurations of one, two or three hidden layers.\n",
    "\n",
    "For one inner layer of 36 units, $N^{(1)}_{max}=10\\times35+36\\times1=386$.\n",
    "\n",
    "For two inner layers, let $d^{(1)}=x$ be the dimension of the first layer (i.e. the number of units minus one). The number of weights is then an analytic function of $x$, specifically:\n",
    "\n",
    "\\begin{equation}\n",
    "N^{(2)}(x)=10x+(x+1)(36-x-2)+(36-x-1)=69 + 42 x - x^2\\,.\n",
    "\\end{equation}\n",
    "\n",
    "This function is maximized for $x_{max}=21$, where $N^{(2)}(x_{max})=N^{(2)}_{max}=510$, which correspond to a first layer of 22 units followed by a second layer of 14 units.\n",
    "\n",
    "For three layers, let $d^{(1)}=x$ and $d^{(2)}=y$ be the dimension of the first and second layer respectively (i.e. the number of units minus one). The number of weights now is the following two-dimensional surface:\n",
    "\n",
    "\\begin{equation}\n",
    "N^{(3)}(x)=10x+(x+1)y+(y+1)(36-x-y-3)+(36-x-y-2)=67 + 8 x + 32 y - y^2\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Using the script below, we obtain that the function is maximized for $x_{max}=20$ and $y_{max}=12$, giving $N^{(3)}_{max}=467$.\n",
    "\n",
    "Hence, the maximum number of weights are $N^{(2)}_{max}=510$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N3max=467 found at d=[9, 20, 12, 1, 1]!\n"
     ]
    }
   ],
   "source": [
    "Nmax=0\n",
    "dmax=[]\n",
    "\n",
    "for i in range(31):\n",
    "    for j in range(33-i):\n",
    "        k=36-(i+j+2)-1\n",
    "        d=[9,i,j,k,1]\n",
    "        if(nweights(d)>Nmax):\n",
    "            Nmax=nweights(d)\n",
    "            dmax=d\n",
    "\n",
    "print(f'N3max={Nmax} found at d={dmax}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
